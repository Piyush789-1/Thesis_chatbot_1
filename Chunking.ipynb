{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec69f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the file path as needed.\n",
    "file_path = r'D:\\ML\\Thesis_chatbot\\Data\\out\\preprocessed_md.md'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    markdown_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7101456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_into_sections(text: str) -> list:\n",
    "    sections = []\n",
    "    lines = text.splitlines()\n",
    "    current_heading = None\n",
    "    current_content = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip lines that are completely blank.\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        \n",
    "        if line.strip().startswith(\"#\"):\n",
    "            # When encountering a heading, flush the previous section, if any.\n",
    "            if current_heading is not None or current_content:\n",
    "                # If there's a heading, add it at the top of the content.\n",
    "                if current_heading:\n",
    "                    full_content = current_heading + \"\\n\" + \"\\n\".join(current_content).strip()\n",
    "                else:\n",
    "                    full_content = \"\\n\".join(current_content).strip()\n",
    "                section = {\n",
    "                    \"heading\": current_heading if current_heading else \"\",\n",
    "                    \"content\": full_content\n",
    "                }\n",
    "                if section[\"heading\"] or section[\"content\"]:\n",
    "                    sections.append(section)\n",
    "                # Reset the content accumulator.\n",
    "                current_content = []\n",
    "            # Update the current heading from the new heading line.\n",
    "            current_heading = line.strip().lstrip(\"#\").strip()\n",
    "        else:\n",
    "            # Accumulate content lines.\n",
    "            current_content.append(line)\n",
    "    \n",
    "    # Flush any remaining content after the loop.\n",
    "    if current_heading is not None or current_content:\n",
    "        if current_heading:\n",
    "            full_content = current_heading + \"\\n\" + \"\\n\".join(current_content).strip()\n",
    "        else:\n",
    "            full_content = \"\\n\".join(current_content).strip()\n",
    "        section = {\n",
    "            \"heading\": current_heading if current_heading else \"\",\n",
    "            \"content\": full_content\n",
    "        }\n",
    "        if section[\"heading\"] or section[\"content\"]:\n",
    "            sections.append(section)\n",
    "    \n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e299a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split the markdown into sections.\n",
    "sections = split_markdown_into_sections(markdown_text)\n",
    "print(\"Total sections found:\", len(sections))\n",
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b38805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Utility Functions\n",
    "def unique_list(l: list) -> list:\n",
    "    \"\"\"Removes duplicates from a list while preserving order.\"\"\"\n",
    "    return list(dict.fromkeys(l))\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Dummy token estimator based on whitespace splits.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "def extract_overlap_text(chunk_text: str, overlap_token_count: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Extract an overlap snippet for continuity. Skips atomic lines like tables, formulas and images.\n",
    "    \"\"\"\n",
    "    lines = chunk_text.splitlines()\n",
    "    selected_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if (stripped.startswith(\"|\") or stripped.startswith(\"$$\") or \n",
    "            stripped.startswith(\"![\") or stripped.startswith(\"TABLE_TITLE:\") or \n",
    "            stripped.startswith(\"TABLE_COLUMNS:\") or stripped.startswith(\"Where:\") or \n",
    "            stripped.startswith(\"-\")):\n",
    "            continue\n",
    "        selected_lines.append(line)\n",
    "    selected_text = \" \".join(selected_lines).strip()\n",
    "    words = selected_text.split()\n",
    "    if len(words) > overlap_token_count:\n",
    "        return \" \".join(words[-overlap_token_count:])\n",
    "    return selected_text\n",
    "\n",
    "def extract_media_metadata(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts media metadata from markdown for images and tables.\n",
    "    Expects images to be marked with ![IMG_TITLE: <caption>] and tables with TABLE_TITLE: markers.\n",
    "    \"\"\"\n",
    "    image_metadata = []\n",
    "    table_metadata = []\n",
    "\n",
    "    # Images via IMG_TITLE marker.\n",
    "    image_pattern = re.compile(r'!\\[IMG_TITLE:\\s*(.*?)\\]')\n",
    "    for match in image_pattern.finditer(text):\n",
    "        image_metadata.append({'title': match.group(1).strip()})\n",
    "\n",
    "    # Tables via TABLE_TITLE markers.\n",
    "    table_title_pattern = re.compile(r'TABLE_TITLE:\\s*(.*)')\n",
    "    for match in table_title_pattern.finditer(text):\n",
    "        table_metadata.append({'title': match.group(1).strip()})\n",
    "    \n",
    "    # Bottom table captions.\n",
    "    table_bottom_pattern = re.compile(r\"(Table\\s+\\d+(?:\\.\\d+)?):\\s*(.+)\")\n",
    "    for match in table_bottom_pattern.finditer(text):\n",
    "        combined_title = f\"{match.group(1).strip()}: {match.group(2).strip()}\"\n",
    "        table_metadata.append({'title': combined_title})\n",
    "    return {'images': image_metadata, 'tables': table_metadata}\n",
    "\n",
    "def unify_metadata(metadata: dict) -> str:\n",
    "    \"\"\"\n",
    "    Converts the structured metadata dictionary (with keys like \"headings\", \"images\", \"tables\")\n",
    "    into a unified string with explicit labels and delimiters.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    if \"headings\" in metadata and metadata[\"headings\"]:\n",
    "        join_headings = \"; \".join(metadata[\"headings\"])\n",
    "        parts.append(\"Headings: \" + join_headings)\n",
    "    if \"images\" in metadata and metadata[\"images\"]:\n",
    "        join_images = \"; \".join(metadata[\"images\"])\n",
    "        parts.append(\"Figures: \" + join_images)\n",
    "    if \"tables\" in metadata and metadata[\"tables\"]:\n",
    "        join_tables = \"; \".join(metadata[\"tables\"])\n",
    "        parts.append(\"Tables: \" + join_tables)\n",
    "    unified_str = \" | \".join(parts)\n",
    "    return unified_str\n",
    "\n",
    "def contains_atomic(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the text contains atomic elements (tables, formulas or images).\n",
    "    \"\"\"\n",
    "    if re.search(r'^\\s*\\|', text, flags=re.MULTILINE):\n",
    "        return True\n",
    "    if \"$$\" in text:\n",
    "        return True\n",
    "    if re.search(r'!\\[', text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def split_text_by_token_limit(text: str, token_limit: int, overlap: int) -> list:\n",
    "    \"\"\"\n",
    "    Splits a (possibly long) text into subchunks where each subchunk's token\n",
    "    count does not exceed token_limit. An overlap is maintained between subchunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    subchunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + token_limit\n",
    "        sub_chunk = \" \".join(words[start:end])\n",
    "        subchunks.append(sub_chunk)\n",
    "        start = max(0, end - overlap)\n",
    "    return subchunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a544fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee6b8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sections_to_chunks(sections: list,\n",
    "                               token_limit_normal: int = 400,\n",
    "                               token_limit_atomic: int = 2000,\n",
    "                               overlap_token_count: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Combines section dictionaries (from split_markdown_into_sections) into chunks.\n",
    "\n",
    "    Requirements:\n",
    "      1. Try to keep chunk size up to token_limit_normal (400 tokens) for any section.\n",
    "      2. If an atomic section is larger than 400 tokens, allow it to extend up to token_limit_atomic (2000 tokens).\n",
    "      3. If even with token_limit_atomic the atomic section is not fully covered, then split it.\n",
    "         In that case, first attempt to split on internal headings. If internal headings are found,\n",
    "         create subchunks based on those splits. Otherwise, fall back to token-based splitting.\n",
    "         Every resulting subchunk carries the section’s heading in its metadata.\n",
    "      4. If a heading appears at the end of a chunk, remove all trailing heading lines and store \n",
    "         them so that they are immediately prepended to the next chunk's content.\n",
    "      5. Finally, if a chunk’s text is identical to its derived overlap text (i.e. it only contains metadata\n",
    "         and overlap text), then the chunk is removed.\n",
    "\n",
    "    A brief overlap snippet (of overlap_token_count tokens) is attached between chunks only if no pending heading exists.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk_sections = []  # Accumulates text for the current chunk.\n",
    "    current_chunk_metadata = {\"headings\": [], \"images\": [], \"tables\": []}\n",
    "    overlap_prefix = \"\"\n",
    "    pending_heading = \"\"  # Stores one or more trailing heading lines to be injected into the next chunk.\n",
    "\n",
    "    def flush_current_chunk() -> str:\n",
    "        nonlocal current_chunk_sections, current_chunk_metadata, pending_heading, overlap_prefix\n",
    "        if not current_chunk_sections:\n",
    "            return \"\"\n",
    "        chunk_text = \"\\n\".join(current_chunk_sections).strip()\n",
    "\n",
    "        # --- New Code: Remove all trailing heading lines ---\n",
    "        lines = chunk_text.splitlines()\n",
    "        heading_lines = []\n",
    "        # Remove lines from the bottom as long as they exactly match one of the accumulated headings.\n",
    "        while lines and (lines[-1].strip() in current_chunk_metadata[\"headings\"]):\n",
    "            heading_lines.append(lines.pop())\n",
    "        # Reconstruct the chunk text without the trailing headings.\n",
    "        chunk_text = \"\\n\".join(lines).strip()\n",
    "        # Save the removed headings (in original order) as pending_heading.\n",
    "        if heading_lines:\n",
    "            pending_heading = \"\\n\".join(reversed(heading_lines))\n",
    "        else:\n",
    "            pending_heading = \"\"\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        # Remove markdown heading markers.\n",
    "        cleaned_text = re.sub(r'^#+\\s*', '', chunk_text, flags=re.MULTILINE)\n",
    "        media_metadata = extract_media_metadata(cleaned_text)\n",
    "        combined_metadata = {\n",
    "            \"headings\": unique_list(current_chunk_metadata[\"headings\"]),\n",
    "            \"images\": unique_list(current_chunk_metadata[\"images\"] + [item['title'] for item in media_metadata.get('images', [])]),\n",
    "            \"tables\": unique_list(current_chunk_metadata[\"tables\"] + [item['title'] for item in media_metadata.get('tables', [])])\n",
    "        }\n",
    "        unified_meta = unify_metadata(combined_metadata)\n",
    "        chunks.append({\n",
    "            \"chunk_text\": cleaned_text,\n",
    "            \"metadata\": unified_meta\n",
    "        })\n",
    "        # If no pending heading is captured, create an overlap snippet;\n",
    "        # otherwise, leave overlap snippet empty (because pending_heading will be injected).\n",
    "        overlap_snippet = \"\" if pending_heading else extract_overlap_text(cleaned_text, overlap_token_count)\n",
    "        current_chunk_sections = []\n",
    "        current_chunk_metadata = {\"headings\": [], \"images\": [], \"tables\": []}\n",
    "        return overlap_snippet\n",
    "\n",
    "    for section in sections:\n",
    "        section_text = section[\"content\"]\n",
    "        section_heading = section[\"heading\"]\n",
    "        section_tokens = estimate_tokens(section_text)\n",
    "        is_atomic = contains_atomic(section_text)\n",
    "\n",
    "        # Process atomic sections that exceed the normal limit.\n",
    "        if is_atomic and section_tokens > token_limit_normal:\n",
    "            if current_chunk_sections:\n",
    "                overlap_prefix = flush_current_chunk()\n",
    "            if section_tokens <= token_limit_atomic:\n",
    "                media_metadata = extract_media_metadata(section_text)\n",
    "                unified_meta = unify_metadata({\n",
    "                    \"headings\": [section_heading] if section_heading else [],\n",
    "                    \"images\": unique_list([item['title'] for item in media_metadata.get('images', [])]),\n",
    "                    \"tables\": unique_list([item['title'] for item in media_metadata.get('tables', [])])\n",
    "                })\n",
    "                chunks.append({\n",
    "                    \"chunk_text\": section_text,\n",
    "                    \"metadata\": unified_meta\n",
    "                })\n",
    "                overlap_prefix = extract_overlap_text(section_text, overlap_token_count)\n",
    "            else:\n",
    "                # Atomic section exceeds token_limit_atomic.\n",
    "                # First try to split based on internal headings.\n",
    "                internal_sections = split_markdown_into_sections(section_text)\n",
    "                if len(internal_sections) > 1:\n",
    "                    for subsec in internal_sections:\n",
    "                        sub_text = subsec[\"content\"]\n",
    "                        if estimate_tokens(sub_text) > token_limit_atomic:\n",
    "                            subchunks = split_text_by_token_limit(sub_text, token_limit_atomic, overlap_token_count)\n",
    "                            for sub in subchunks:\n",
    "                                media_metadata = extract_media_metadata(sub)\n",
    "                                unified_meta = unify_metadata({\n",
    "                                    \"headings\": [section_heading] if section_heading else [],\n",
    "                                    \"images\": unique_list([item['title'] for item in media_metadata.get('images', [])]),\n",
    "                                    \"tables\": unique_list([item['title'] for item in media_metadata.get('tables', [])])\n",
    "                                })\n",
    "                                chunks.append({\n",
    "                                    \"chunk_text\": sub,\n",
    "                                    \"metadata\": unified_meta\n",
    "                                })\n",
    "                        else:\n",
    "                            media_metadata = extract_media_metadata(sub_text)\n",
    "                            unified_meta = unify_metadata({\n",
    "                                \"headings\": [section_heading] if section_heading else [],\n",
    "                                \"images\": unique_list([item['title'] for item in media_metadata.get('images', [])]),\n",
    "                                \"tables\": unique_list([item['title'] for item in media_metadata.get('tables', [])])\n",
    "                            })\n",
    "                            chunks.append({\n",
    "                                \"chunk_text\": sub_text,\n",
    "                                \"metadata\": unified_meta\n",
    "                            })\n",
    "                    overlap_prefix = \"\"\n",
    "                else:\n",
    "                    # Fallback: split purely by token count.\n",
    "                    subchunks = split_text_by_token_limit(section_text, token_limit_atomic, overlap_token_count)\n",
    "                    for sub in subchunks:\n",
    "                        media_metadata = extract_media_metadata(sub)\n",
    "                        unified_meta = unify_metadata({\n",
    "                            \"headings\": [section_heading] if section_heading else [],\n",
    "                            \"images\": unique_list([item['title'] for item in media_metadata.get('images', [])]),\n",
    "                            \"tables\": unique_list([item['title'] for item in media_metadata.get('tables', [])])\n",
    "                        })\n",
    "                        chunks.append({\n",
    "                            \"chunk_text\": sub,\n",
    "                            \"metadata\": unified_meta\n",
    "                        })\n",
    "                    overlap_prefix = \"\"\n",
    "            continue  # Move to next section.\n",
    "\n",
    "        # Process non-atomic sections (or atomic sections within token_limit_normal).\n",
    "        if not current_chunk_sections:\n",
    "            # Always inject pending_heading (if it exists) into every new chunk,\n",
    "            # regardless of any overlap_prefix.\n",
    "            if pending_heading:\n",
    "                current_chunk_sections.append(pending_heading)\n",
    "                pending_heading = \"\"\n",
    "            if overlap_prefix:\n",
    "                current_chunk_sections.append(overlap_prefix)\n",
    "                overlap_prefix = \"\"\n",
    "        tentative_chunk = (\"\\n\".join(current_chunk_sections) + \"\\n\" if current_chunk_sections else \"\") + section_text\n",
    "        if estimate_tokens(tentative_chunk) <= token_limit_normal:\n",
    "            current_chunk_sections.append(section_text)\n",
    "            if section_heading:\n",
    "                current_chunk_metadata[\"headings\"].append(section_heading)\n",
    "        else:\n",
    "            overlap_prefix = flush_current_chunk()\n",
    "            if overlap_prefix:\n",
    "                current_chunk_sections.append(overlap_prefix)\n",
    "            current_chunk_sections.append(section_text)\n",
    "            if section_heading:\n",
    "                current_chunk_metadata[\"headings\"].append(section_heading)\n",
    "\n",
    "    if current_chunk_sections:\n",
    "        flush_current_chunk()\n",
    "\n",
    "    # --- Filter trivial chunks ---\n",
    "    filtered_chunks = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk[\"chunk_text\"].strip()\n",
    "        overlap_snippet = extract_overlap_text(chunk_text, overlap_token_count).strip()\n",
    "        if chunk_text != overlap_snippet:\n",
    "            filtered_chunks.append(chunk)\n",
    "    # -------------------------------\n",
    "\n",
    "    return filtered_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6260e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Combine the section dictionaries into chunks with extended metadata.\n",
    "metadata_chunks = combine_sections_to_chunks(\n",
    "    sections,\n",
    "    token_limit_normal=400,\n",
    "    token_limit_atomic=2000,\n",
    "    overlap_token_count=50\n",
    ")\n",
    "print(\"Total chunks generated:\", len(metadata_chunks))\n",
    "print(\"\\n----- Metadata of Chunks -----\\n\")\n",
    "for idx, meta in enumerate(metadata_chunks):\n",
    "    print(f\"--- Chunk {idx+1} ---\")\n",
    "    print(\"Metadata:\", meta['metadata'])\n",
    "    print(\"Chunk Text:\\n\", meta['chunk_text'])\n",
    "    print(\"\\n---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d4d51fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metadata chunks saved to D:\\ML\\Thesis_chatbot\\Data\\out\\metadata_chunks.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_output_path = r\"D:\\ML\\Thesis_chatbot\\Data\\out\\metadata_chunks.json\"\n",
    "with open(json_output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(metadata_chunks, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ Metadata chunks saved to {json_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
