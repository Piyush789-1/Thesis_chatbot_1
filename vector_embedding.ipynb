{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b9fe885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer model constructed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Construct a SentenceTransformer model manually using the local model\n",
    "\n",
    "\n",
    "# Define the local model path \n",
    "local_model_path = \"./models/sentence_transformer_all_mpnet_base_v2\"\n",
    "\n",
    "# Create a transformer model instance using the local model path.\n",
    "# This module helps in building token level embeddings for each word in input sentences.\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "transformer_model = models.Transformer(\n",
    "    model_name_or_path=local_model_path,\n",
    "    tokenizer_args={\"local_files_only\": True}\n",
    ")\n",
    "\n",
    "# Create a pooling model that builds on top of the transformer model\n",
    "# This module aggregates the token level embeddings from transformer_model into a single vector for each input sentence.\n",
    "pooling_model = models.Pooling(transformer_model.get_word_embedding_dimension())\n",
    "\n",
    "# Combine the modules into a complete SentenceTransformer model.\n",
    "# Here we assemble the transformer and pooling models into a single SentenceTransformer instance. \n",
    "# we will use this model to encode sentences into fixed-size embeddings.(768 dimensions) \n",
    "model = SentenceTransformer(modules=[transformer_model, pooling_model])\n",
    "\n",
    "print(\"SentenceTransformer model constructed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d928e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model is loaded on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check for GPU availability and move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Embedding model is loaded on: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ae384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample sentence\n",
    "sample_text = [\"This is a sample sentence to test the embedding model.\"]\n",
    "embedding = model.encode(sample_text)\n",
    "\n",
    "print(\"Embedding shape:\", embedding.shape)\n",
    "print(\"Embedding vector:\", embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "337d4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''purposeof this script is to have the vector represenation of each sentance in the form of list of lists.\n",
    "* For input documents, if single sentance or multiple sentances, the output will be a list of lists where each inner\n",
    "   list is the vector representation of the corresponding input sentance.\n",
    "*  While for input query, the output will be a single list representing the vector of the query.\n",
    "  This is useful for various NLP tasks such as semantic search, clustering, and classification.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class CustomSentenceTransformerEmbeddings:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        ''' If a single string is accidentally passed, wrap it in a list.\n",
    "        here if a texts is a single string, we convert it to a list with one element.\n",
    "        This ensures that the model can handle it uniformly as a list of texts.'''\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "        \n",
    "        '''If the result is a 1D array (should not happen for a list of texts) then wrap it.\n",
    "        here the array is converted from a single list to, list of lists.. ex. [0.1, 0.2, ...]   converts to [[0.1, 0.2, ...]]\n",
    "        '''\n",
    "        if embeddings.ndim == 1:\n",
    "            return [embeddings.tolist()]\n",
    "        \n",
    "        # If it's a 2D array, return the list of lists directly. no need to wrap it in list as output will be a list of lists.\n",
    "        elif embeddings.ndim == 2:\n",
    "            return embeddings.tolist()\n",
    "        else:\n",
    "            # Fallback, though typically not needed.\n",
    "            return [emb.tolist() for emb in embeddings]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        embedding = self.model.encode(text, convert_to_numpy=True)\n",
    "        # If the embedding comes back as a 2D array (e.g., shape [1, d]), get the first element.\n",
    "        # Here  we ensure that if the embedding is a 2D array with one row, we convert it to a 1D list. ex. [[0.1, 0.2, ...]] converts to [0.1, 0.2, ...]\n",
    "        if isinstance(embedding, np.ndarray):\n",
    "            if embedding.ndim == 2:\n",
    "                embedding = embedding[0]\n",
    "            return embedding.tolist()\n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd355e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function=CustomSentenceTransformerEmbeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66557a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load JSON content\n",
    "with open(r\"D:\\ML\\Thesis_chatbot\\Data\\out\\metadata_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list of Document objects with both page content and metadata\n",
    "documents = [\n",
    "    Document(page_content=item[\"chunk_text\"], metadata={\"metadata\": item[\"metadata\"]})\n",
    "    for item in data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Path where you want to store the Chroma DB\n",
    "persist_directory = r\"D:\\ML\\Thesis_chatbot\\Data\\out\\chroma_db\"\n",
    "\n",
    "# Create the vector store using your custom embedding function\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=persist_directory,  \n",
    "    collection_name=\"my_collection\"       \n",
    ")\n",
    "vectorstore.persist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample query\n",
    "query = \"What OEM names are mentioned?\"\n",
    "\n",
    "# Retrieve the top 3 similar documents\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "# Examine the results\n",
    "for doc in results:\n",
    "    print(\"Text snippet:\", doc.page_content[:500])\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
